import torch
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

from model import GPSDRecommender
from data_preprocessing import SequenceDataset

class RecommendationEvaluator:
    """æ¨èç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self, model: GPSDRecommender, device: torch.device):
        self.model = model.to(device)
        self.device = device
        self.model.eval()
    
    def predict_top_k(self, 
                     input_ids: torch.Tensor, 
                     attention_mask: torch.Tensor,
                     k: int = 10,
                     exclude_seen: bool = True) -> torch.Tensor:
        """é¢„æµ‹Top-Kæ¨èç‰©å“"""
        with torch.no_grad():
            logits = self.model(input_ids, attention_mask)
            
            if exclude_seen:
                # æ’é™¤å·²ç»äº¤äº’è¿‡çš„ç‰©å“
                for i in range(input_ids.size(0)):
                    seen_items = input_ids[i][input_ids[i] != 0]  # æ’é™¤padding
                    logits[i, seen_items] = float('-inf')
            
            # è·å–Top-K
            _, top_k_items = torch.topk(logits, k, dim=1)
            
        return top_k_items
    
    def calculate_recall_at_k(self, 
                             predictions: List[List[int]], 
                             ground_truth: List[List[int]], 
                             k: int = 10) -> float:
        """è®¡ç®—Recall@K"""
        total_recall = 0
        valid_users = 0
        
        for pred, truth in zip(predictions, ground_truth):
            if len(truth) == 0:
                continue
                
            pred_set = set(pred[:k])
            truth_set = set(truth)
            
            recall = len(pred_set & truth_set) / len(truth_set)
            total_recall += recall
            valid_users += 1
        
        return total_recall / valid_users if valid_users > 0 else 0.0
    
    def calculate_precision_at_k(self, 
                                predictions: List[List[int]], 
                                ground_truth: List[List[int]], 
                                k: int = 10) -> float:
        """è®¡ç®—Precision@K"""
        total_precision = 0
        valid_users = 0
        
        for pred, truth in zip(predictions, ground_truth):
            if len(truth) == 0:
                continue
                
            pred_set = set(pred[:k])
            truth_set = set(truth)
            
            if len(pred_set) > 0:
                precision = len(pred_set & truth_set) / len(pred_set)
                total_precision += precision
                valid_users += 1
        
        return total_precision / valid_users if valid_users > 0 else 0.0
    
    def calculate_ndcg_at_k(self, 
                           predictions: List[List[int]], 
                           ground_truth: List[List[int]], 
                           k: int = 10) -> float:
        """è®¡ç®—NDCG@K"""
        def dcg_at_k(relevances: List[float], k: int) -> float:
            """è®¡ç®—DCG@K"""
            dcg = 0
            for i, rel in enumerate(relevances[:k]):
                dcg += rel / np.log2(i + 2)  # i+2 because log2(1) = 0
            return dcg
        
        total_ndcg = 0
        valid_users = 0
        
        for pred, truth in zip(predictions, ground_truth):
            if len(truth) == 0:
                continue
                
            truth_set = set(truth)
            
            # è®¡ç®—é¢„æµ‹ç»“æœçš„ç›¸å…³æ€§
            relevances = [1.0 if item in truth_set else 0.0 for item in pred[:k]]
            
            # è®¡ç®—DCG
            dcg = dcg_at_k(relevances, k)
            
            # è®¡ç®—IDCGï¼ˆç†æƒ³DCGï¼‰
            ideal_relevances = [1.0] * min(len(truth), k)
            idcg = dcg_at_k(ideal_relevances, k)
            
            # è®¡ç®—NDCG
            ndcg = dcg / idcg if idcg > 0 else 0.0
            total_ndcg += ndcg
            valid_users += 1
        
        return total_ndcg / valid_users if valid_users > 0 else 0.0
    
    def calculate_hit_rate_at_k(self, 
                               predictions: List[List[int]], 
                               ground_truth: List[List[int]], 
                               k: int = 10) -> float:
        """è®¡ç®—Hit Rate@K"""
        hits = 0
        valid_users = 0
        
        for pred, truth in zip(predictions, ground_truth):
            if len(truth) == 0:
                continue
                
            pred_set = set(pred[:k])
            truth_set = set(truth)
            
            if len(pred_set & truth_set) > 0:
                hits += 1
            valid_users += 1
        
        return hits / valid_users if valid_users > 0 else 0.0
    
    def calculate_coverage(self, 
                          predictions: List[List[int]], 
                          total_items: int,
                          k: int = 10) -> float:
        """è®¡ç®—æ¨èè¦†ç›–ç‡"""
        recommended_items = set()
        
        for pred in predictions:
            recommended_items.update(pred[:k])
        
        return len(recommended_items) / total_items
    
    def calculate_diversity(self, 
                           predictions: List[List[int]], 
                           item_features: Optional[Dict[int, List]] = None,
                           k: int = 10) -> float:
        """è®¡ç®—æ¨èå¤šæ ·æ€§ï¼ˆåŸºäºç‰©å“ç‰¹å¾çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰"""
        if item_features is None:
            # å¦‚æœæ²¡æœ‰ç‰©å“ç‰¹å¾ï¼Œä½¿ç”¨ç®€å•çš„å¤šæ ·æ€§åº¦é‡
            total_diversity = 0
            valid_users = 0
            
            for pred in predictions:
                if len(pred) < 2:
                    continue
                    
                # è®¡ç®—æ¨èåˆ—è¡¨ä¸­ç‰©å“çš„å”¯ä¸€æ€§
                unique_items = len(set(pred[:k]))
                diversity = unique_items / min(len(pred), k)
                total_diversity += diversity
                valid_users += 1
            
            return total_diversity / valid_users if valid_users > 0 else 0.0
        
        # åŸºäºç‰©å“ç‰¹å¾çš„å¤šæ ·æ€§è®¡ç®—
        # è¿™é‡Œå¯ä»¥æ‰©å±•å®ç°æ›´å¤æ‚çš„å¤šæ ·æ€§åº¦é‡
        return 0.0
    
    def evaluate_model(self, 
                      test_sequences: List[Tuple[List[int], int]],
                      k_values: List[int] = [5, 10, 20],
                      batch_size: int = 64) -> Dict[str, float]:
        """å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        
        print("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_dataset = SequenceDataset(test_sequences, self.model.backbone.max_seq_len)
        test_loader = torch.utils.data.DataLoader(
            test_dataset, batch_size=batch_size, shuffle=False
        )
        
        all_predictions = []
        all_ground_truth = []
        
        # ç”Ÿæˆé¢„æµ‹
        with torch.no_grad():
            for batch in tqdm(test_loader, desc="ç”Ÿæˆé¢„æµ‹"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                targets = batch['target'].cpu().numpy()
                
                # é¢„æµ‹Top-K
                max_k = max(k_values)
                top_k_items = self.predict_top_k(input_ids, attention_mask, k=max_k)
                
                # æ”¶é›†é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
                for i in range(len(targets)):
                    pred = top_k_items[i].cpu().numpy().tolist()
                    truth = [targets[i]] if targets[i] != 0 else []  # æ’é™¤padding
                    
                    all_predictions.append(pred)
                    all_ground_truth.append(truth)
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        results = {}
        
        for k in k_values:
            results[f'Recall@{k}'] = self.calculate_recall_at_k(all_predictions, all_ground_truth, k)
            results[f'Precision@{k}'] = self.calculate_precision_at_k(all_predictions, all_ground_truth, k)
            results[f'NDCG@{k}'] = self.calculate_ndcg_at_k(all_predictions, all_ground_truth, k)
            results[f'HitRate@{k}'] = self.calculate_hit_rate_at_k(all_predictions, all_ground_truth, k)
        
        # è®¡ç®—è¦†ç›–ç‡å’Œå¤šæ ·æ€§
        results['Coverage@10'] = self.calculate_coverage(all_predictions, self.model.n_items, k=10)
        results['Diversity@10'] = self.calculate_diversity(all_predictions, k=10)
        
        return results
    
    def compare_models(self, 
                      baseline_results: Dict[str, float],
                      current_results: Dict[str, float]) -> Dict[str, float]:
        """æ¯”è¾ƒæ¨¡å‹æ€§èƒ½"""
        improvements = {}
        
        for metric in current_results:
            if metric in baseline_results:
                baseline_val = baseline_results[metric]
                current_val = current_results[metric]
                
                if baseline_val != 0:
                    improvement = (current_val - baseline_val) / baseline_val * 100
                    improvements[f'{metric}_improvement_%'] = improvement
                else:
                    improvements[f'{metric}_improvement_%'] = float('inf') if current_val > 0 else 0
        
        return improvements
    
    def simulate_ab_test(self, 
                        test_sequences: List[Tuple[List[int], int]],
                        baseline_gmv: float = 1000.0,
                        expected_improvement: float = 7.97) -> Dict[str, float]:
        """æ¨¡æ‹ŸA/Bæµ‹è¯•GMVæå‡"""
        
        # è¯„ä¼°æ¨¡å‹æ€§èƒ½
        results = self.evaluate_model(test_sequences)
        
        # åŸºäºRecall@10ä¼°ç®—GMVæå‡
        recall_10 = results.get('Recall@10', 0)
        
        # ç®€åŒ–çš„GMVæå‡æ¨¡å‹ï¼šå‡è®¾Recallæå‡ç›´æ¥è½¬åŒ–ä¸ºGMVæå‡
        # å®é™…æƒ…å†µä¸­éœ€è¦æ›´å¤æ‚çš„ä¸šåŠ¡æ¨¡å‹
        estimated_gmv_improvement = recall_10 * expected_improvement
        
        new_gmv = baseline_gmv * (1 + estimated_gmv_improvement / 100)
        
        ab_test_results = {
            'baseline_gmv': baseline_gmv,
            'new_gmv': new_gmv,
            'gmv_improvement_%': estimated_gmv_improvement,
            'recall@10': recall_10,
            'expected_improvement_%': expected_improvement
        }
        
        return ab_test_results
    
    def plot_evaluation_results(self, 
                               results: Dict[str, float],
                               save_path: str = "./results/evaluation_results.png"):
        """ç»˜åˆ¶è¯„ä¼°ç»“æœ"""
        
        # åˆ†ç¦»ä¸åŒç±»å‹çš„æŒ‡æ ‡
        recall_metrics = {k: v for k, v in results.items() if 'Recall' in k}
        precision_metrics = {k: v for k, v in results.items() if 'Precision' in k}
        ndcg_metrics = {k: v for k, v in results.items() if 'NDCG' in k}
        other_metrics = {k: v for k, v in results.items() 
                        if not any(x in k for x in ['Recall', 'Precision', 'NDCG'])}
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # RecallæŒ‡æ ‡
        if recall_metrics:
            k_values = [int(k.split('@')[1]) for k in recall_metrics.keys()]
            recall_values = list(recall_metrics.values())
            axes[0, 0].plot(k_values, recall_values, 'o-', color='blue', linewidth=2, markersize=8)
            axes[0, 0].set_title('Recall@K', fontsize=14, fontweight='bold')
            axes[0, 0].set_xlabel('K')
            axes[0, 0].set_ylabel('Recall')
            axes[0, 0].grid(True, alpha=0.3)
            axes[0, 0].set_ylim(0, max(recall_values) * 1.1)
        
        # NDCGæŒ‡æ ‡
        if ndcg_metrics:
            k_values = [int(k.split('@')[1]) for k in ndcg_metrics.keys()]
            ndcg_values = list(ndcg_metrics.values())
            axes[0, 1].plot(k_values, ndcg_values, 'o-', color='green', linewidth=2, markersize=8)
            axes[0, 1].set_title('NDCG@K', fontsize=14, fontweight='bold')
            axes[0, 1].set_xlabel('K')
            axes[0, 1].set_ylabel('NDCG')
            axes[0, 1].grid(True, alpha=0.3)
            axes[0, 1].set_ylim(0, max(ndcg_values) * 1.1)
        
        # PrecisionæŒ‡æ ‡
        if precision_metrics:
            k_values = [int(k.split('@')[1]) for k in precision_metrics.keys()]
            precision_values = list(precision_metrics.values())
            axes[1, 0].plot(k_values, precision_values, 'o-', color='red', linewidth=2, markersize=8)
            axes[1, 0].set_title('Precision@K', fontsize=14, fontweight='bold')
            axes[1, 0].set_xlabel('K')
            axes[1, 0].set_ylabel('Precision')
            axes[1, 0].grid(True, alpha=0.3)
            axes[1, 0].set_ylim(0, max(precision_values) * 1.1)
        
        # å…¶ä»–æŒ‡æ ‡
        if other_metrics:
            metric_names = list(other_metrics.keys())
            metric_values = list(other_metrics.values())
            
            bars = axes[1, 1].bar(range(len(metric_names)), metric_values, 
                                 color=['orange', 'purple', 'brown', 'pink'][:len(metric_names)])
            axes[1, 1].set_title('Other Metrics', fontsize=14, fontweight='bold')
            axes[1, 1].set_xticks(range(len(metric_names)))
            axes[1, 1].set_xticklabels(metric_names, rotation=45, ha='right')
            axes[1, 1].set_ylabel('Value')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, metric_values):
                axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                               f'{value:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        import os
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"è¯„ä¼°ç»“æœå›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")

def print_evaluation_report(results: Dict[str, float], 
                           ab_test_results: Optional[Dict[str, float]] = None):
    """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
    
    print("\n" + "="*50)
    print("           æ¨èç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š")
    print("="*50)
    
    # ç¦»çº¿æŒ‡æ ‡
    print("\nğŸ“Š ç¦»çº¿è¯„ä¼°æŒ‡æ ‡:")
    print("-" * 30)
    
    for metric, value in results.items():
        if '@' in metric:
            print(f"{metric:15s}: {value:.4f}")
        else:
            print(f"{metric:15s}: {value:.4f}")
    
    # A/Bæµ‹è¯•ç»“æœ
    if ab_test_results:
        print("\nğŸ’° A/Bæµ‹è¯•GMVæ¨¡æ‹Ÿ:")
        print("-" * 30)
        
        for metric, value in ab_test_results.items():
            if 'gmv' in metric.lower():
                if '%' in metric:
                    print(f"{metric:20s}: {value:.2f}%")
                else:
                    print(f"{metric:20s}: ${value:,.2f}")
            else:
                print(f"{metric:20s}: {value:.4f}")
    
    print("\n" + "="*50)

if __name__ == "__main__":
    # ç¤ºä¾‹è¯„ä¼°
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹å’Œæ•°æ®
    from model import GPSDRecommender
    
    model = GPSDRecommender(n_items=1000)
    evaluator = RecommendationEvaluator(model, device)
    
    # åˆ›å»ºæµ‹è¯•åºåˆ—
    test_sequences = [(list(range(1, 11)), 11) for _ in range(100)]
    
    # è¯„ä¼°æ¨¡å‹
    results = evaluator.evaluate_model(test_sequences)
    
    # A/Bæµ‹è¯•æ¨¡æ‹Ÿ
    ab_results = evaluator.simulate_ab_test(test_sequences)
    
    # æ‰“å°æŠ¥å‘Š
    print_evaluation_report(results, ab_results)
    
    # ç»˜åˆ¶ç»“æœ
    evaluator.plot_evaluation_results(results)